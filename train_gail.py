import os
import torch as th
import torch.nn as nn
from tqdm.auto import tqdm
import numpy as np

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.logger import configure

from imitation.algorithms.adversarial.gail import GAIL
from imitation.rewards.reward_nets import BasicRewardNet

from load_expert_data import load_and_convert_data
from bomberman_gym import BombermanEnv
import gym


# None ê´€ì¸¡ê°’ ë°©ì–´ ë˜í¼
class NanGuardWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
    
    def observation(self, obs):
        if obs is None:
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        return np.array(obs, dtype=np.float32)

def make_env():
    env = BombermanEnv()
    env = NanGuardWrapper(env)
    env = Monitor(env)
    return env

# 2. Residual CNN
class ResidualBlock(nn.Module):
    def __init__(self, channels: int):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),
        )
        self.activation = nn.ReLU()

    def forward(self, x: th.Tensor) -> th.Tensor:
        out = self.block(x)
        out = out + x  # skip connection
        return self.activation(out)


class BombermanCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        n_input_channels = observation_space.shape[0]

        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),                          # (32, 17, 17)

            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),                          # (64, 9, 9)

            ResidualBlock(64),                  # (64, 9, 9)
            ResidualBlock(64),                  # (64, 9, 9)

            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),                          # (64, 5, 5)

            ResidualBlock(64),                  # (64, 5, 5)

            nn.Flatten(),
        )

        with th.no_grad():
            sample_input = th.as_tensor(observation_space.sample()[None]).float()
            n_flatten = self.cnn(sample_input).shape[1]

        self.linear = nn.Sequential(
            nn.Linear(n_flatten, features_dim),
            nn.ReLU(),
        )

    def forward(self, observations: th.Tensor) -> th.Tensor:
        return self.linear(self.cnn(observations))


# í™˜ê²½ ì„¤ì •
print("ğŸš€ Initializing Bomberman GAIL Training...")
env = DummyVecEnv([make_env])

# ì „ë¬¸ê°€ ë°ì´í„° ë¡œë“œ
print("ğŸ“Š Loading expert trajectories...")
trajectories = load_and_convert_data("dataset/expert_raw")
print(f"âœ… Loaded {len(trajectories)} trajectories")

# ë³´ìƒ ë„¤íŠ¸ì›Œí¬
reward_net = BasicRewardNet(
    observation_space=env.observation_space,
    action_space=env.action_space,
    normalize_input_layer=None,
)

# PPO Generator
print("ğŸ¤– Setting up PPO Generator...")
learner = PPO(
    env=env,
    policy="CnnPolicy",
    batch_size=64,
    n_steps=256,
    ent_coef=0.01,
    learning_rate=3e-4,
    n_epochs=8,
    verbose=1,
    policy_kwargs=dict(
        features_extractor_class=BombermanCNN,
        features_extractor_kwargs=dict(features_dim=256),
        normalize_images=False,
    ),
)

# TensorBoard ë¡œê¹…
logger = configure("./tb_logs/", ["stdout", "tensorboard", "csv"])
learner.set_logger(logger)

# GAIL Trainer
print("âš”ï¸ Initializing GAIL Trainer...")
gail_trainer = GAIL(
    demonstrations=trajectories,
    demo_batch_size=512,
    gen_replay_buffer_capacity=4096,
    n_disc_updates_per_round=16,
    venv=env,
    gen_algo=learner,
    reward_net=reward_net,
    allow_variable_horizon=True, 
)

# [ìˆ˜ì •] ì•ˆì „í•œ ì§„í–‰ ì½œë°±
class SafeProgressCallback(BaseCallback):
    def __init__(self, total_steps, verbose=0):
        super().__init__(verbose)
        self.total_steps = total_steps
        
    def _on_step(self) -> bool:
        if self.num_timesteps % 1000 == 0 and self.num_timesteps > 0:
            progress = (self.num_timesteps / self.total_steps) * 100
            print(f"ğŸ”„ Progress: {self.num_timesteps:,}/{self.total_steps:,} "
                  f"({progress:.1f}%) | FPS: {self.locals.get('fps', 0):.0f}")
        return True

# ì²´í¬í¬ì¸íŠ¸ + ì§„í–‰ ì½œë°±
checkpoint_callback = CheckpointCallback(
    save_freq=5000,
    save_path="./checkpoints/",
    name_prefix="gail_bomberman",
    verbose=1,
)

progress_callback = SafeProgressCallback(total_steps=20_000)

# 9. í•™ìŠµ ì‹œì‘!
TOTAL_STEPS = 100_000
print(f"\nğŸ¯ Starting GAIL Training for {TOTAL_STEPS:,} timesteps...")
print("ğŸ“ˆ TensorBoard: tensorboard --logdir tb_logs/")
print("ğŸ’¾ Checkpoints: ./checkpoints/ (manual save recommended)")
print("=" * 80)

print("âš¡ PPO ê¸°ë³¸ ë¡œê·¸ë¡œ ì‹¤ì‹œê°„ ì§„í–‰ í™•ì¸ ì¤‘...")
gail_trainer.train(total_timesteps=TOTAL_STEPS)  # ì½œë°± ì œê±°!

print("ğŸ’¾ Emergency Save: í˜„ì¬ ìƒíƒœ ì €ì¥ ì¤‘...")
learner.save("gail_bomberman_emergency")
print("âœ… Emergency model saved: gail_bomberman_emergency.zip")
gail_trainer.reward_net.save("emergency_reward_net")
print("âœ… Reward net also saved!")